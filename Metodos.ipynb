{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEk+6LDW3ZVHBaZA4TFwBI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lpaolariosm/Estad-stica-/blob/main/Metodos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avxAbCAV39eF",
        "outputId": "8bcd1add-4afe-49bd-ef98-10794e9b4fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 1: x = [ 1.8 -2.6], grad = [ 2. -4.], v = [ 0.2 -0.4]\n",
            "Iter 2: x = [ 1.496 -2.144], grad = [ 1.24 -0.96], v = [ 0.304 -0.456]\n",
            "Iter 3: x = [ 1.17792 -1.84016], grad = [0.4448 1.0656], v = [ 0.31808 -0.30384]\n",
            "Iter 4: x = [ 0.9133184 -1.7400224], grad = [-0.216704  1.733184], v = [ 0.2646016 -0.1001376]\n",
            "Iter 5: x = [ 0.74014157 -1.78993914], grad = [-0.64964608  1.40040576], v = [0.17317683 0.04991674]\n",
            "Iter 6: x = [ 0.66742594 -1.90091852], grad = [-0.83143516  0.66054321], v = [0.07271563 0.11097938]\n",
            "Iter 7: x = [ 0.68158549 -2.00047998], grad = [-0.79603627 -0.00319986], v = [-0.01415956  0.09956146]\n",
            "Iter 8: x = [ 0.75546328 -2.05405117], grad = [-0.61134181 -0.36034117], v = [-0.07387778  0.0535712 ]\n",
            "Iter 9: x = [ 0.85756262 -2.06135915], grad = [-0.35609344 -0.40906101], v = [-0.10209935  0.00730798]\n",
            "Iter 10: x = [ 0.95956163 -2.0407618 ], grad = [-0.10109592 -0.27174532], v = [-0.10199901 -0.02059735]\n",
            "Iter 11: x = [ 1.04108859 -2.01333451], grad = [ 0.10272147 -0.08889672], v = [-0.08152696 -0.02742729]\n",
            "Iter 12: x = [ 1.09157028 -1.99318997], grad = [0.2289257  0.04540021], v = [-0.05048169 -0.02014454]\n",
            "Iter 13: x = [ 1.10960304 -1.98503593], grad = [0.27400761 0.09976047], v = [-0.01803276 -0.00815404]\n",
            "Iter 14: x = [ 1.10066602 -1.98661838], grad = [0.25166506 0.08921082], v = [0.00893702 0.00158245]\n",
            "Iter 15: x = [ 1.07409816 -1.99282555], grad = [0.18524541 0.04782968], v = [0.02656786 0.00620717]\n",
            "Iter 16: x = [ 1.04014967 -1.9990472 ], grad = [0.10037418 0.00635199], v = [0.03394849 0.00622165]\n",
            "Iter 17: x = [ 1.00767682 -2.00278801], grad = [ 0.01919206 -0.01858675], v = [0.03247285 0.00374081]\n",
            "Iter 18: x = [ 0.98276101 -2.00369285], grad = [-0.04309748 -0.02461898], v = [0.02491582 0.00090483]\n",
            "Iter 19: x = [ 0.96826942 -2.00270432], grad = [-0.07932645 -0.01802879], v = [ 0.01449159 -0.00098853]\n",
            "Iter 20: x = [ 0.96418159 -2.00108879], grad = [-0.08954602 -0.00725857], v = [ 0.00408783 -0.00161553]\n",
            "\n",
            "Solución encontrada: [ 0.96418159 -2.00108879]\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "import numpy as np\n",
        "\n",
        "# Definir la función simbólicamente\n",
        "x, y = sp.symbols('x y')\n",
        "f = (x - 1)**2 + 2*(y + 2)**2  # ejemplo de función convexa\n",
        "\n",
        "# Calcular el gradiente simbólico\n",
        "grad_f = [sp.diff(f, var) for var in (x, y)]\n",
        "\n",
        "# Convertir el gradiente a función numérica\n",
        "grad_f_num = sp.lambdify((x, y), grad_f, 'numpy')\n",
        "\n",
        "# Función de descenso acelerado (Nesterov)\n",
        "def nesterov_accelerated_gradient(grad_func, lr=0.1, momentum=0.9, x0=np.array([0.0, 0.0]), max_iter=100):\n",
        "    x_prev = x0\n",
        "    v = np.zeros_like(x0)\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        lookahead = x_prev - momentum * v\n",
        "        grad = np.array(grad_func(*lookahead))\n",
        "        v = momentum * v + lr * grad\n",
        "        x_prev = x_prev - v\n",
        "\n",
        "        print(f\"Iter {i+1}: x = {x_prev}, grad = {grad}, v = {v}\")\n",
        "\n",
        "    return x_prev\n",
        "\n",
        "# Ejecutar el método\n",
        "solution = nesterov_accelerated_gradient(grad_f_num, lr=0.1, momentum=0.9, x0=np.array([2.0, -3.0]), max_iter=20)\n",
        "\n",
        "print(\"\\nSolución encontrada:\", solution)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "import numpy as np\n",
        "\n",
        "# Definir variables simbólicas\n",
        "x, y = sp.symbols('x y')\n",
        "vars = sp.Matrix([x, y])\n",
        "\n",
        "# Definir la función objetivo simbólicamente (ejemplo)\n",
        "f = (x - 2)**4 + (x - 2*y)**2\n",
        "\n",
        "# Gradiente y Hessiana\n",
        "grad_f = sp.Matrix([sp.diff(f, var) for var in vars])\n",
        "hessian_f = grad_f.jacobian(vars)\n",
        "\n",
        "# Convertir gradiente y Hessiana a funciones numéricas\n",
        "grad_func = sp.lambdify((x, y), grad_f, 'numpy')\n",
        "hessian_func = sp.lambdify((x, y), hessian_f, 'numpy')\n",
        "\n",
        "# Método de Newton\n",
        "def newton_method(grad_func, hessian_func, x0, tol=1e-6, max_iter=50):\n",
        "    xk = np.array(x0, dtype=float)\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad_val = np.array(grad_func(*xk), dtype=float).flatten()\n",
        "        hess_val = np.array(hessian_func(*xk), dtype=float)\n",
        "\n",
        "        if np.linalg.norm(grad_val) < tol:\n",
        "            print(f\"Convergencia en la iteración {i}\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            delta = np.linalg.solve(hess_val, grad_val)\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(\"Hessiana no invertible en la iteración\", i)\n",
        "            break\n",
        "\n",
        "        xk = xk - delta\n",
        "        print(f\"Iter {i+1}: x = {xk}, grad = {grad_val}, ||grad|| = {np.linalg.norm(grad_val)}\")\n",
        "\n",
        "    return xk\n",
        "\n",
        "# Ejecutar el método\n",
        "x0 = [0.0, 0.0]\n",
        "sol = newton_method(grad_func, hessian_func, x0)\n",
        "print(\"\\nSolución encontrada:\", sol)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EiVFdtv9MNf",
        "outputId": "a8d4e441-ff7a-428a-aa78-0abe2f590eb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 1: x = [0.66666667 0.33333333], grad = [-32.   0.], ||grad|| = 32.0\n",
            "Iter 2: x = [1.11111111 0.55555556], grad = [-9.48148148  0.        ], ||grad|| = 9.48148148148148\n",
            "Iter 3: x = [1.40740741 0.7037037 ], grad = [-2.80932785  0.        ], ||grad|| = 2.8093278463648828\n",
            "Iter 4: x = [1.60493827 0.80246914], grad = [-0.83239344  0.        ], ||grad|| = 0.8323934359599653\n",
            "Iter 5: x = [1.73662551 0.86831276], grad = [-0.24663509  0.        ], ||grad|| = 0.246635092136286\n",
            "Iter 6: x = [1.82441701 0.9122085 ], grad = [-0.07307706  0.        ], ||grad|| = 0.07307706433667728\n",
            "Iter 7: x = [1.88294467 0.94147234], grad = [-0.02165246  0.        ], ||grad|| = 0.021652463507163665\n",
            "Iter 8: x = [1.92196312 0.96098156], grad = [-0.00641554  0.        ], ||grad|| = 0.006415544742863308\n",
            "Iter 9: x = [1.94797541 0.97398771], grad = [-0.0019009  0.       ], ||grad|| = 0.0019009021460335836\n",
            "Iter 10: x = [1.96531694 0.98265847], grad = [-0.00056323  0.        ], ||grad|| = 0.0005632302654914346\n",
            "Iter 11: x = [1.97687796 0.98843898], grad = [-0.00016688  0.        ], ||grad|| = 0.0001668830416270928\n",
            "Iter 12: x = [1.98458531 0.99229265], grad = [-4.94468271e-05  0.00000000e+00], ||grad|| = 4.944682714876966e-05\n",
            "Iter 13: x = [1.98972354 0.99486177], grad = [-1.46509117e-05  0.00000000e+00], ||grad|| = 1.465091174778318e-05\n",
            "Iter 14: x = [1.99314903 0.99657451], grad = [-4.34101089e-06  0.00000000e+00], ||grad|| = 4.341010888231679e-06\n",
            "Iter 15: x = [1.99543268 0.99771634], grad = [-1.28622545e-06  0.00000000e+00], ||grad|| = 1.2862254483651502e-06\n",
            "Convergencia en la iteración 15\n",
            "\n",
            "Solución encontrada: [1.99543268 0.99771634]\n"
          ]
        }
      ]
    }
  ]
}